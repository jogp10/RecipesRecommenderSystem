{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hummus - Community Based Recommendations**\n",
    "Notebook for the first project for the Machine Learning Complements course (CAC).\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "The goal of this project is to develop a recommendation system for the Hummus dataset. The dataset contains information about users and their ratings for different recipes. The system should be able to recommend items to users based on their reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following libraries will be used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities, girvan_newman, label_propagation_communities, fast_label_propagation_communities\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import Dataset, Reader, KNNBasic, NormalPredictor, SVD, accuracy\n",
    "import utils as ut\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "In this section of the notebook, we define all the constants that will be used throughout the code. Constants are values that do not change and remain the same every time the code is run. Defining constants at the beginning of the code makes it easier to manage and modify them if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True # Logging\n",
    "SAMPLES = 10000 # Number of samples to use, only applies if USE_SAMPLES = False\n",
    "USE_SAMPLES = True # If False, retrieve data from full dataset\n",
    "USE_STORED_GRAPH = False # If False, generate new graph, otherwise load from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Our dataset was collected from food.com website. It contains information about users and their ratings/reviews for different recipes. The dataset is stored in 3 CSV files, which we will load into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SAMPLES:\n",
    "    df_members = pd.read_csv('pp_members_sampled.csv')\n",
    "    df_recipes = pd.read_csv('pp_recipes_sampled.csv')\n",
    "    df_reviews = pd.read_csv('pp_reviews_sampled.csv')\n",
    "else:\n",
    "    df_members = pd.read_csv('pp_members.csv')#, nrows=SAMPLES)\n",
    "    df_recipes = pd.read_csv('pp_recipes.csv')#, nrows=SAMPLES)\n",
    "    df_reviews = pd.read_csv('pp_reviews.csv', nrows=SAMPLES)\n",
    "\n",
    "\n",
    "    df_members = df_members[df_members['member_id'].isin(df_reviews['member_id'])] # keep only members who have reviewed\n",
    "    df_recipes = df_recipes[df_recipes['recipe_id'].isin(df_reviews['recipe_id'])] # keep only recipes that have been reviewed\n",
    "    \n",
    "    # Save the sampled data\n",
    "    df_members.to_csv('pp_members_sampled.csv', index=False)\n",
    "    df_recipes.to_csv('pp_recipes_sampled.csv', index=False)\n",
    "    df_reviews.to_csv('pp_reviews_sampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Oberservations\n",
    "\n",
    "The dataset contains 3 files: `recipes.csv`, `users.csv`, and `reviews.csv`. The `recipes.csv` file contains information about the recipes, such as the recipe name, the recipe id, and the recipe ingredients. The `users.csv` file contains information about the users, such as the user id and the user name. The `reviews.csv` file contains information about the reviews, such as the user id, the recipe id, and the rating given by the user to the recipe.\n",
    "\n",
    "In this section we will take a look at the first few rows of each file to get a better understanding of the data, and do some initial data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Observation - Members dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.initial_obs(df_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_members.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Observation - Recipes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.initial_obs(df_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Observation - Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.initial_obs(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews distribution across rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 95% of the reviews are positive, with a rating of 4 or 5. This is important to keep in mind when building the recommendation system, as it may be biased towards recommending popular recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_reviews_rating(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of number of reviews per user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly one review per user, which is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_users_num_reviews(df):\n",
    "    reviews_count = df.groupby('member_id')['review_id'].count()\n",
    "\n",
    "    # Define bins for the histogram\n",
    "    bins = [0, 5, 10, 15, reviews_count.max()+1]  # Bins for both ranges\n",
    "    labels = ['1-5', '6-10', '11-15', '16-'+str(reviews_count.max())]  # Labels for bins\n",
    "\n",
    "    # Calculate frequencies for each range\n",
    "    frequencies = [((reviews_count > bins[i]) & (reviews_count <= bins[i+1])).sum() for i in range(len(bins)-1)]\n",
    "\n",
    "    # Plot the distribution of ratings\n",
    "    bars = plt.bar(labels, frequencies, color='skyblue', edgecolor='black')\n",
    "\n",
    "    # Plot bar plot with pre-calculated frequencies\n",
    "    plt.bar(labels, frequencies, color='skyblue', edgecolor='black')\n",
    "    plt.bar_label(plt.bar(labels, frequencies, color='skyblue', edgecolor='black'))\n",
    "\n",
    "    plt.title('Distribution of Number of Reviews per User')\n",
    "    plt.xlabel('Number of Reviews')\n",
    "    plt.ylabel('Number of Members')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_num_users_num_reviews(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 most popular reviewed recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Recipes with more than 20 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rating for each recipe\n",
    "\n",
    "# Filter recipes with more than 20 reviews\n",
    "filtered_recipes = df_recipes[df_recipes['number_of_ratings'] > 20]\n",
    "\n",
    "# Sort recipes based on average rating\n",
    "top_rated_recipes = filtered_recipes.sort_values(by='average_rating', ascending=False).head(10)\n",
    "\n",
    "# Print the name and rating of the top-rated recipes as well as the number of reviews\n",
    "print('Top-Rated Recipes:')\n",
    "print('------------------')\n",
    "\n",
    "for index, recipe in top_rated_recipes.iterrows():\n",
    "    print(f\"{recipe['title']} (Recipe ID: {recipe['recipe_id']}) - Average Rating: {recipe['average_rating']:.2f} ({recipe['number_of_ratings']} reviews)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finishing the Initial Observations\n",
    "\n",
    "In this section we have loaded the data, explored the first few rows of each file, and done some initial data exploration. We have also identified some key points that will be important to keep in mind when building the recommendation system.\n",
    "\n",
    "- 90% of our reviews are positive, with a rating of 4 or 5.\n",
    "- Most users have only reviewed one recipe.\n",
    "- The dataset was collected and preprocessed before being provided to us, so there is no need for further preprocessing at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Preparation - Create the graph for network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by doing a Social Network Analysis on the dataset. This will help us understand the relationships present in our dataset and finding patterns that can be used to make recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting by creating a graph with the members, as our main focus, as nodes and the reviews as edges. The weight of the edges will be the number of reviews in common (to the same recipe with the same sentiment-feeling) between the two members. This will allow us to use network analysis to find communities of members with similar tastes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will group the reviews by recipe and evaluations, so we can extract the members that have connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group reviews by recipe and evaluation (>3, <=3)\n",
    "grouped_reviews = df_reviews.groupby(['recipe_id', df_reviews['rating'] > 3])\n",
    "\n",
    "# Create a dictionary to store relations between users\n",
    "user_relations = {}\n",
    "\n",
    "# Iterate through each group\n",
    "for (recipe_id, is_positive_rating), group in grouped_reviews:\n",
    "    # Extract user IDs for this recipe and evaluation\n",
    "    if VERBOSE: print(recipe_id, is_positive_rating, group['member_id'].unique())\n",
    "    user_ids = group['member_id'].unique()\n",
    "    user_ids.sort()\n",
    "    \n",
    "    # Update relations between users for this recipe\n",
    "    for i, user_id1 in enumerate(user_ids):\n",
    "        for user_id2 in user_ids[i+1:]:\n",
    "            # Check if there's an entry for this relation between users\n",
    "            if (user_id1, user_id2) not in user_relations:\n",
    "                if VERBOSE: print(f\"Creating new relation between {user_id1} and {user_id2}\")\n",
    "                user_relations[(user_id1, user_id2)] = 0\n",
    "            \n",
    "            # Increment the relation count between the users based on the evaluation\n",
    "            user_relations[(user_id1, user_id2)] += 2 if is_positive_rating else 1\n",
    "            if VERBOSE: print(f\"Relation between {user_id1} and {user_id2} has been incremented to {user_relations[(user_id1, user_id2)]}\")\n",
    "\n",
    "# Now user_relations contains relations between users\n",
    "if VERBOSE: print(\"Size of user_relations:\", len(user_relations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users with the same taste will have a high number in the relation, and users with different tastes will have a low number. \n",
    "\n",
    "We decided to give more weight on the positive reviews, as they defined more the taste of the users. (2 times more weight on positive reviews)\n",
    "\n",
    "Here are the most strong relations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted(user_relations.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Convert the sorted_dict to a DataFrame and split the tuple into two columns\n",
    "sorted_df = pd.DataFrame(sorted_dict, columns=['member_ids', 'relationship_strength'])\n",
    "sorted_df[['member_id1', 'member_id2']] = pd.DataFrame(sorted_df['member_ids'].tolist(), index=sorted_df.index)\n",
    "sorted_df = sorted_df.drop(columns='member_ids')\n",
    "\n",
    "# Merge with the members DataFrame to get the names for member_id1\n",
    "merged_df = pd.merge(sorted_df, df_members[['member_id', 'member_name']], left_on='member_id1', right_on='member_id')\n",
    "\n",
    "# Rename the member_name column to member_name1\n",
    "merged_df = merged_df.rename(columns={'member_name': 'member_name1'})\n",
    "\n",
    "# Merge again with the members DataFrame to get the names for member_id2\n",
    "merged_df = pd.merge(merged_df, df_members[['member_id', 'member_name']], left_on='member_id2', right_on='member_id')\n",
    "\n",
    "# Rename the member_name column to member_name2\n",
    "merged_df = merged_df.rename(columns={'member_name': 'member_name2'})\n",
    "\n",
    "# Print the 10 pairs with the strongest relationships along with their names and IDs\n",
    "for index, row in merged_df[:10].iterrows():\n",
    "    print((row['member_id1'], row['member_name1']), (row['member_id2'], row['member_name2']), \":\", row['relationship_strength'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "vertex_indices = {}\n",
    "\n",
    "# Check if the file exists\n",
    "if USE_STORED_GRAPH and os.path.exists('graph_file.graphml'):\n",
    "    # Load the graph from file\n",
    "    if VERBOSE: print(\"Loading graph from file\")\n",
    "    g = nx.read_graphml('graph_file.graphml')\n",
    "else:\n",
    "    if VERBOSE: print(\"Creating new graph\")\n",
    "    for (u,v), weight in user_relations.items():\n",
    "        g.add_edge(u, v, weight = weight)\n",
    "    nx.write_graphml(g, \"graph_file.graphml\")\n",
    "    \n",
    "if VERBOSE: print(g)\n",
    "\n",
    "connected_components = list(nx.connected_components(g))\n",
    "largest_connected_component = max(connected_components, key=len)\n",
    "g = g.subgraph(largest_connected_component)\n",
    "\n",
    "# Use the spring layout\n",
    "pos = nx.spring_layout(g)\n",
    "\n",
    "# Draw the graph without labels\n",
    "nx.draw(g, pos, with_labels=False, node_size=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network Analysis\n",
    "\n",
    "Before we start using models we will do some reasearch on our recently created network graph.\n",
    "\n",
    "Some data about the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of members:\", g.number_of_nodes())\n",
    "print(\"Number of connections:\", g.number_of_edges())\n",
    "print(\"Average degree:\", sum(dict(g.degree()).values()) / g.number_of_nodes())\n",
    "print(\"Graph density:\", nx.density(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph itself is very sparse as the density is very low.\n",
    "\n",
    "### Power Law Distribution\n",
    "\n",
    "Here, we will investigate whether our network adheres to a power law distribution, which signifies a characteristic pattern in which a few nodes possess an exceptionally high number of connections, while the majority have only a few connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = sorted([d for n, d in g.degree()], reverse=True)\n",
    "degree_count = np.unique(degree_sequence, return_counts=True)\n",
    "\n",
    "# Plot degree distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(degree_count[0], degree_count[1], marker='o', color='b', alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Degree Distribution\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our network does follow a power law distribution. However, there are some outliers that might appear because we're only looking at a portion of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Worlds Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small worlds theory posits that within complex networks, such as social networks or neural connections, most nodes can be reached from any other node via a small number of steps, highlighting the prevalence of short path lengths and high clustering coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eccentricities = nx.eccentricity(g)\n",
    "\n",
    "#avg_shortest_path_length = nx.average_shortest_path_length(g)\n",
    "#print(avg_shortest_path_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see by the results, it takes approximately 3 nodes (small) to reach one node from any other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diameter of a graph is the length of the shortest path between the most distanced nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter = max(eccentricities.values())\n",
    "print(f\"Diameter of the graph: {diameter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_nodes = [node for node, eccentricity in eccentricities.items() if eccentricity == diameter]\n",
    "source_node = center_nodes[0]\n",
    "target_node = center_nodes[1]\n",
    "diameter_path = nx.shortest_path(g, source=source_node, target=target_node)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(g)  # Positions for all nodes\n",
    "\n",
    "# Draw nodes not in the diameter path\n",
    "non_diameter_nodes = [node for node in g.nodes() if node not in diameter_path]\n",
    "nx.draw_networkx_nodes(g, pos, nodelist=non_diameter_nodes, node_size=50, alpha=0.2)\n",
    "\n",
    "# Draw non-diameter edges\n",
    "non_diameter_edges = [edge for edge in g.edges() if edge not in [(diameter_path[i], diameter_path[i + 1]) for i in range(len(diameter_path) - 1)]]\n",
    "nx.draw_networkx_edges(g, pos, edgelist=non_diameter_edges, width=1, alpha=0.2)\n",
    "\n",
    "# Draw nodes in the diameter path with bigger size\n",
    "nx.draw_networkx_nodes(g, pos, nodelist=diameter_path, node_size=200, node_color=\"red\")\n",
    "\n",
    "# Draw diameter path with thicker line width\n",
    "diameter_edges = [(diameter_path[i], diameter_path[i + 1]) for i in range(len(diameter_path) - 1)]\n",
    "nx.draw_networkx_edges(g, pos, edgelist=diameter_edges, edge_color='red', width=2)\n",
    "\n",
    "\n",
    "\n",
    "plt.title(f\"Graph Diameter: {diameter}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Influencial Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll employ various statistical measures to extract insights about our data, particularly focusing on identifying influential users. To achieve this, we will compute different centrality metrics including degree centrality, betweenness centrality, eigenvector centrality, PageRank and closeness centrality for the top 10 users in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user with high degree centrality likely reviews a large number of recipes. They may be very active in providing feedback on various recipes, indicating a strong engagement with the platform or community. They might have a significant influence on others in the network, potentially influencing their choices of recipes for others to try.\n",
    "Degree Centrality measures how connected a node is by counting its direct connections, akin to popularity in a social network, where individuals with more friends are considered more central."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_USERS = 10\n",
    "\n",
    "degree_centrality = nx.degree_centrality(g)\n",
    "degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "degree_centrality_members = df_members[df_members['member_id'].isin([int(x[0]) for x in degree_centrality[0:5]])]\n",
    "\n",
    "# Add number of reviews to the top users\n",
    "degree_centrality_members = degree_centrality_members.merge(df_reviews.groupby('member_id').size().reset_index(name='number_of_reviews'), on='member_id')\n",
    "\n",
    "degree_centrality_members = degree_centrality_members[['member_id', 'member_name', 'number_of_reviews', 'follow_me_count']]\n",
    "degree_centrality_members['number_of_connections'] = [g.degree(x[0]) for x in degree_centrality[0:5]]\n",
    "\n",
    "# Add number of connections to the DataFrame\n",
    "degree_centrality_members['degree_centrality'] = [x[1] for x in degree_centrality[0:5]]\n",
    "degree_centrality_members.head(AMOUNT_USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closeness Centrality quantifies how quickly a node can interact with others, similar to being centrally located in a city where reaching any destination is efficient due to proximity. This measure calculates the average length of the shortest path from a node to all other nodes in the graph. In your case, a user with high closeness centrality can reach all other users through a short path of shared recipe reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closeness_centrality = nx.closeness_centrality(g, distance='weight')\n",
    "# closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# closeness_centrality_members = df_members[df_members['member_id'].isin([int(x[0]) for x in closeness_centrality[0:5]])]\n",
    "\n",
    "# # Add number of reviews to the top users\n",
    "# closeness_centrality_members = closeness_centrality_members.merge(df_reviews.groupby('member_id').size().reset_index(name='number_of_reviews'), on='member_id')\n",
    "\n",
    "# closeness_centrality_members = closeness_centrality_members[['member_id', 'member_name', 'number_of_reviews', 'follow_me_count]]\n",
    "# closeness_centrality_members['number_of_connections'] = [g.degree(x[0]) for x in closeness_centrality[0:5]]\n",
    "\n",
    "# # Add closeness centrality to the DataFrame\n",
    "# closeness_centrality_members['closeness_centrality'] = [x[1] for x in closeness_centrality[0:5]]\n",
    "\n",
    "# closeness_centrality_members.head(AMOUNT_USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweenness Centrality\n",
    "\n",
    "Betweenness Centrality evaluates the importance of a node in facilitating communication between other nodes, resembling a bridge in a network where traffic flows through it, making it crucial for connectivity. This measure calculates the number of shortest paths from all nodes to all others that pass through a node. In your case, a user with high betweenness centrality often acts as a bridge between other users' recipe reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betweenness_centrality = nx.betweenness_centrality(g)\n",
    "# betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# betweenness_centrality_members = df_members[df_members['member_id'].isin([int(x[0]) for x in betweenness_centrality[0:5]])]\n",
    "\n",
    "# # Add number of reviews to the top users\n",
    "# betweenness_centrality_members = betweenness_centrality_members.merge(df_reviews.groupby('member_id').size().reset_index(name='number_of_reviews'), on='member_id')\n",
    "\n",
    "# betweenness_centrality_members = betweenness_centrality_members[['member_id', 'member_name', 'number_of_reviews', 'follow_me_count]]\n",
    "# betweenness_centrality_members['number_of_connections'] = [g.degree(x[0]) for x in betweenness_centrality[0:5]]\n",
    "\n",
    "# # Add betweenness centrality to the DataFrame\n",
    "# betweenness_centrality_members['betweenness_centrality'] = [x[1] for x in betweenness_centrality[0:5]]\n",
    "\n",
    "# betweenness_centrality_members.head(AMOUNT_USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigen Centrality\n",
    "\n",
    "Eigen centrality assesses a node's importance by considering not only its direct connections but also the importance of its neighbors, similar to a ripple effect where influence spreads from influential nodes throughout the network. This measure calculates a node's influence based on the number of links it has to other influential nodes. In your case, a user with high eigenvector centrality has reviewed many of the same recipes as other influential users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_centrality = nx.eigenvector_centrality(g, weight='weight')\n",
    "eigen_centrality = sorted(eigen_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "eigen_centrality_members = df_members[df_members['member_id'].isin([int(x[0]) for x in eigen_centrality[0:5]])]\n",
    "\n",
    "# Add number of reviews to the top users\n",
    "eigen_centrality_members = eigen_centrality_members.merge(df_reviews.groupby('member_id').size().reset_index(name='number_of_reviews'), on='member_id')\n",
    "\n",
    "eigen_centrality_members = eigen_centrality_members[['member_id', 'member_name', 'number_of_reviews', 'follow_me_count']]\n",
    "eigen_centrality_members['number_of_connections'] = [g.degree(x[0]) for x in eigen_centrality[0:5]]\n",
    "\n",
    "# Add eigenvector centrality to the DataFrame\n",
    "eigen_centrality_members['eigen_centrality'] = [x[1] for x in eigen_centrality[0:5]]\n",
    "\n",
    "eigen_centrality_members.head(AMOUNT_USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page Rank\n",
    "\n",
    "PageRank algorithm, assesses a node's importance based on the quantity and quality of its connections. Originally used by Google to rank websites, it's similar to eigenvector centrality, but it involves a damping factor which represents the probability of a user explore reviews from users they are connected to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(g, weight='weight')\n",
    "pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pagerank_members = df_members[df_members['member_id'].isin([int(x[0]) for x in pagerank[0:5]])]\n",
    "\n",
    "# Add number of reviews to the top users\n",
    "pagerank_members = pagerank_members.merge(df_reviews.groupby('member_id').size().reset_index(name='number_of_reviews'), on='member_id')\n",
    "\n",
    "pagerank_members = pagerank_members[['member_id', 'member_name', 'number_of_reviews', 'follow_me_count']]\n",
    "pagerank_members['number_of_connections'] = [g.degree(x[0]) for x in pagerank[0:5]]\n",
    "\n",
    "# Add PageRank to the DataFrame\n",
    "pagerank_members['pagerank'] = [x[1] for x in pagerank[0:5]]\n",
    "\n",
    "pagerank_members.head(AMOUNT_USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITS (Hubs and Authorities)\n",
    "\n",
    "HITS algorithm, also known as hubs and authorities, evaluates the importance of a node based on two factors: its authority, which is the number of hubs that point to it, and its hub value, which is the number of authorities it points to. This algorithm is particularly useful for identifying the most influential nodes in a network.\n",
    "\n",
    "As we are working with an undirected graph, we will see equal values for both hubs and authorities, so we will only show the authorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HITS algorithm\n",
    "hubs, authorities = nx.hits(g)\n",
    "\n",
    "sorted_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create DataFrames for the top 5 hubs and authorities\n",
    "authorities_members = df_members[df_members['member_id'].isin([int(x[0]) for x in sorted_authorities[0:5]])]\n",
    "\n",
    "# Add number of reviews to the top users\n",
    "authorities_members = authorities_members.merge(df_reviews.groupby('member_id').size().reset_index(name='number_of_reviews'), on='member_id')\n",
    "\n",
    "authorities_members = authorities_members[['member_id', 'member_name', 'number_of_reviews', 'follow_me_count']]\n",
    "\n",
    "authorities_members['number_of_connections'] = [g.degree(x[0]) for x in sorted_authorities[0:5]]\n",
    "\n",
    "authorities_members['authority_score'] = [x[1] for x in sorted_authorities[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_members.head(AMOUNT_USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some users are present in all the top 5 lists, which means they are very influential in the network. This is important to keep in mind when building the recommendation system, as these users may have a significant impact on the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the dataframes\n",
    "all_metrics_df = pd.concat([\n",
    "    degree_centrality_members, \n",
    "    # closeness_centrality_members, \n",
    "    # betweenness_centrality_members, \n",
    "    eigen_centrality_members, \n",
    "    pagerank_members,\n",
    "    authorities_members\n",
    "    ])\n",
    "\n",
    "# Count the occurrences of each user\n",
    "user_counts = all_metrics_df['member_id'].value_counts()\n",
    "most_influential_users = user_counts[user_counts == user_counts.max()]\n",
    "\n",
    "# Convert most_influential_users to DataFrame\n",
    "most_influential_users_df = most_influential_users.reset_index()\n",
    "most_influential_users_df.columns = ['member_id', 'count']\n",
    "\n",
    "# Merge with df_members to get user names\n",
    "most_influential_users_names = pd.merge(most_influential_users_df, df_members[['member_id', 'member_name', 'follow_me_count', 'member_joined']], on='member_id', how='left')\n",
    "\n",
    "print(most_influential_users_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rich get richer\n",
    "\n",
    "In this section, we will investigate the dilema rich-get-richer in our network. By analyzing the member_joined date we will expect to observe some behaviour on the 'aging' of the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check date distribution across years, display the quartils 25%, 50%, 75%\n",
    "print(df_members['member_joined'].quantile([.25, .5, .75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a common behaviour in social networks, where the more time a user is in the network, the more connections he will have, as it will be more influencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will the reviews of the most influential users be on the top 20 recipes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top-Rated Recipes:')\n",
    "print('------------------')\n",
    "\n",
    "for index, recipe in top_rated_recipes.iterrows():\n",
    "    print(f\"{recipe['title']} (Recipe ID: {recipe['recipe_id']}) - Average Rating: {recipe['average_rating']:.2f} ({recipe['number_of_ratings']} reviews)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews of the most influential users\n",
    "most_influential_users_reviews = df_reviews[df_reviews['member_id'].isin(most_influential_users_names['member_id'])]\n",
    "\n",
    "# Find the recipes that match with top_rated_recipes and print their id, name, avg_rating and number of reviews\n",
    "top_rated_recipes = top_rated_recipes[['recipe_id', 'title', 'average_rating', 'number_of_ratings']]\n",
    "most_influential_users_reviews = most_influential_users_reviews.merge(top_rated_recipes, on='recipe_id')\n",
    "\n",
    "most_influential_users_reviews[['recipe_id', 'title', 'average_rating', 'number_of_ratings']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes!!! Our prediction was correct. We conclude the most influential users have their influence in our network as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection\n",
    "\n",
    "Now we will detect communities in our network. Communities are groups of nodes that are more densely connected to each other than to the rest of the network. Detecting communities can help us identify groups of users with similar tastes, which can be useful for making recommendations. We will assess the algorithm's performance by calculating a few metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Louvain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_communities = greedy_modularity_communities(g, weight='weight')\n",
    "for i, community in enumerate(louvain_communities):\n",
    "    print(f\"Community {i + 1}: {len(community)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Propagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prop_communities = list(fast_label_propagation_communities(g, weight='weight'))\n",
    "label_prop_communities = sorted(label_prop_communities, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "for i, community in enumerate(label_prop_communities):\n",
    "    print(f\"Community {i + 1}: {len(community)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models evaluation\n",
    "\n",
    "We will evaluate the performance of the models by calculating the modularity score. The modularity measures the strength of the division of the network into communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community.quality import modularity\n",
    "\n",
    "louvain_modularity_score = modularity(g, louvain_communities) if len(louvain_communities) > 0 else 0\n",
    "print(\"Louvain Modularity:\", louvain_modularity_score)\n",
    "\n",
    "label_modularity_score = modularity(g, label_prop_communities) if len(label_prop_communities) > 0 else 0\n",
    "print(\"Label Propagation Modularity:\", label_modularity_score)\n",
    "\n",
    "communities = louvain_communities if louvain_modularity_score > label_modularity_score else label_prop_communities\n",
    "print(\"Using communities from Louvain algorithm\" if louvain_modularity_score > label_modularity_score else \"Using communities from Label Propagation algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Filtering\n",
    "Here we will be removing the communities with very few users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average number of users in a community\n",
    "average_users = sum([len(x) for x in communities]) / len(communities)\n",
    "print(\"Average Amount Users p/ Community: \", average_users)\n",
    "\n",
    "filtered_communities = [c for c in communities if len(c) >= average_users]\n",
    "for i, community in enumerate(filtered_communities):\n",
    "    print(f\"Community {i + 1}: {len(community)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Communities\n",
    "\n",
    "We will visualize the communities the algorithms found. Each community will have a different color for better visualization. They will be displayed, firstly, all together and then separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nodes = [node for community in filtered_communities for node in community]\n",
    "\n",
    "# Create a subgraph with only the nodes in the filtered connected component\n",
    "sg = g.subgraph(filtered_nodes)\n",
    "\n",
    "# Assign a color to each community\n",
    "community_colors = [plt.cm.rainbow(i/len(filtered_communities)) for i in range(len(filtered_communities))]\n",
    "\n",
    "# Create a list of colors, one for each node in the filtered connected component\n",
    "colors = []\n",
    "for node in filtered_nodes:\n",
    "    for i, community in enumerate(filtered_communities):\n",
    "        if node in community:\n",
    "            # Add the color to the list of colors\n",
    "            colors.append(community_colors[i])\n",
    "            break\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(15, 15))\n",
    "nx.draw_networkx(sg, with_labels=False, node_color=colors, node_size=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows and columns for the grid\n",
    "n = len(filtered_communities)\n",
    "cols = int(np.ceil(np.sqrt(n)))\n",
    "rows = int(np.ceil(n / cols))\n",
    "\n",
    "# Create a figure and axes for the grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "# Assign a color to each community\n",
    "# community_colors = [plt.cm.rainbow(i/n) for i in range(n)]\n",
    "\n",
    "for i, community in enumerate(filtered_communities):\n",
    "    # Create a subgraph with only the nodes in the current community\n",
    "    sg = g.subgraph(community)\n",
    "\n",
    "    # Create a list of colors, one for each node in the community\n",
    "    colors = [community_colors[i]] * len(community)\n",
    "\n",
    "    # Draw the graph on the corresponding axes\n",
    "    ax = axs[i // cols, i % cols]\n",
    "    ax.set_title(f\"Community {i + 1}\")\n",
    "    nx.draw_networkx(sg, with_labels=False, node_color=colors, node_size=50, ax=ax)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(n, rows*cols):\n",
    "    fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified communities of users with similar tastes, we can use this information to build a recommendation system. We will use the communities to make recommendations to users based on the recipes that other users in the same community have reviewed positively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_rmse = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "communities_mae = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "communities_precision = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "communities_recall = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "whole_dataset_mae = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "whole_dataset_rmse = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "whole_dataset_precision = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "whole_dataset_recall = {\"Random Recommender\":0, \"User-Based CF\":0, \"Item-Based CF\":0, \"Model-Based CF\":0, \"Content-Based Filtering\":0}\n",
    "\n",
    "models_predictions = {}\n",
    "\n",
    "filtered_users = [int(user) for sublist in filtered_communities for user in sublist]\n",
    "\n",
    "df_members = df_members[df_members['member_id'].isin(filtered_users)]\n",
    "df_reviews = df_reviews[df_reviews['member_id'].isin(filtered_users)]\n",
    "df_recipes = df_recipes[df_recipes['recipe_id'].isin(df_reviews['recipe_id'])]\n",
    "\n",
    "print(\"Shape of Filtered Members:\", df_members.shape)\n",
    "print(\"Shape of Filtered Reviews:\", df_reviews.shape)\n",
    "print(\"Shape of Filtered Recipes\", df_recipes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering (Applied @ each community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be exploring recommender systems that help suggest items based on similarities between users or items. We'll dive into both user-based and item-based collaborative filtering methods. Our aim is to apply these techniques to different communities, assess how well they work for each, and then gauge their overall performance by averaging the errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will predict a user's preferences based on the preferences of similar users (users in the same community)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rmse, avg_mae, avg_precision, avg_recall = ut.collaborative_filtering(df_reviews, filtered_communities, model_type='KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage RMSE ->\\033[0m\", avg_rmse)\n",
    "print(f\"\\033[1mAverage MAE ->\\033[0m\", avg_mae)\n",
    "print(f\"\\033[1mAverage Precision@10 ->\\033[0m\", avg_precision)\n",
    "print(f\"\\033[1mAverage Recall@10 ->\\033[0m\", avg_recall)\n",
    "print()    \n",
    "\n",
    "communities_rmse[\"User-Based CF\"] = avg_rmse\n",
    "communities_mae[\"User-Based CF\"] = avg_mae\n",
    "communities_precision[\"User-Based CF\"] = avg_precision\n",
    "communities_recall[\"User-Based CF\"] = avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Item-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use a recommendation approach that predicts a user's preferences by examining similarities between items rather than users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rmse, avg_mae, avg_precision, avg_recall = ut.collaborative_filtering(df_reviews, filtered_communities, user_based=False, model_type='KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage RMSE ->\\033[0m\", avg_rmse)\n",
    "print(f\"\\033[1mAverage MAE ->\\033[0m\", avg_mae)\n",
    "print(f\"\\033[1mAverage Precision@10 ->\\033[0m\", avg_precision)\n",
    "print(f\"\\033[1mAverage Recall@10 ->\\033[0m\", avg_recall)\n",
    "print()    \n",
    "\n",
    "communities_rmse[\"Item-Based CF\"] = avg_rmse\n",
    "communities_mae[\"Item-Based CF\"] = avg_mae\n",
    "communities_precision[\"Item-Based CF\"] = avg_precision\n",
    "communities_recall[\"Item-Based CF\"] = avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is very sparse, so this models are not expected to be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold-Start Problem\n",
    "\n",
    "In collaborative filtering approaches we can face the cold-start problem, where there may be insufficient data about users or items to make accurate recommendations.\n",
    "\n",
    "#### Popularity model (Naive Approach)\n",
    "\n",
    "To tackle the cold-start problem, we will use a popularity model. This model will recommend the most popular recipes to users who do not have enough reviews to make personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by \"average_rating\" and number_of_ratings > 30\n",
    "df_recipes_top = df_recipes[df_recipes['number_of_ratings'] > 30]\n",
    "df_recipes_top = df_recipes_top.sort_values(by='average_rating', ascending=False)\n",
    "\n",
    "# Get top N recommendations\n",
    "top_n_popularity = df_recipes_top.head(10)\n",
    "\n",
    "# Print the top N recommended items\n",
    "print(\"\\nTop Recommendations using Popularity Model:\")\n",
    "for index, recipe in top_n_popularity.iterrows():\n",
    "    print(f\"ID: {recipe['recipe_id']}, Title: {recipe['title']}, Average Rating: {recipe['average_rating']:.2f}, Number of Ratings: {recipe['number_of_ratings']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ model-based collaborative filtering for personalized recommendations, contrasting with memory-based methods. Unlike memory-based approaches that directly compare user-item interactions, model-based methods utilize mathematical models to capture underlying patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rmse, avg_mae, avg_precision, avg_recall = ut.collaborative_filtering(df_reviews, filtered_communities, model_type='SVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage RMSE ->\\033[0m\", avg_rmse)\n",
    "print(f\"\\033[1mAverage MAE ->\\033[0m\", avg_mae)\n",
    "print(f\"\\033[1mAverage Precision@10 ->\\033[0m\", avg_precision)\n",
    "print(f\"\\033[1mAverage Recall@10 ->\\033[0m\", avg_recall)\n",
    "print()    \n",
    "\n",
    "communities_rmse[\"Model-Based CF\"] = avg_rmse\n",
    "communities_mae[\"Model-Based CF\"] = avg_mae\n",
    "communities_precision[\"Model-Based CF\"] = avg_precision\n",
    "communities_recall[\"Model-Based CF\"] = avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content-based Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll be exploring recommender systems that suggest items based on similarities between the characteristics of items. We'll delve into content-based filtering methods, which recommend items to users based on the similarity of the items' features or attributes. Our aim is to apply these techniques to different communities, assess how well they work for each community, and then evaluate their overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By vectorizing text based features we can find similar recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recommendations = ut.find_similars(df_reviews, df_recipes)\n",
    "\n",
    "for recipe_id, recipe_data in all_recommendations.items():\n",
    "    print(f\"\\033[1mOriginal Recipe: {recipe_data['original_title']} (Recipe ID: {recipe_id})\\033[0m\")\n",
    "    print(\"Similar Recipes:\")\n",
    "    unique_similar_recipe_ids = set()  # Track unique similar recipe IDs for each original recipe\n",
    "    for similar_recipe in recipe_data['similar_recipes']:\n",
    "        if similar_recipe['id'] not in unique_similar_recipe_ids:\n",
    "            print(f\"- {similar_recipe['title']} (Recipe ID: {similar_recipe['id']}) | Similarity Score: {similar_recipe['score']:.2f}\")\n",
    "            unique_similar_recipe_ids.add(similar_recipe['id'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similar_recipes = ut.create_similar_recipes_dataframe(all_recommendations)\n",
    "print(df_similar_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_similar_recipes_per_recipe, avg_similar_score = ut.calculate_average_similarity(df_similar_recipes)\n",
    "print(f\"\\033[1mAverage Similar Recipes per Recipe:\\033[0m {avg_similar_recipes_per_recipe:.2f}\")\n",
    "print(f\"\\033[1mAverage Similarity Score:\\033[0m {avg_similar_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen there few pairs of similar recipes, which should translate to a small number of recomendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy, avg_recall = ut.content_based_filtering(df_reviews, df_similar_recipes, filtered_communities)\n",
    "print(f\"\\033[1mAverage Precision ->\\033[0m\", avg_precision)\n",
    "print(f\"\\033[1mAverage Recall ->\\033[0m\", avg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite an high similarity between ratings given by the users to similar recipes, it is rare that the same user reviews similar recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content-based User Profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = ut.user_profiles(filtered_communities, df_reviews, df_recipes, df_members)\n",
    "print(user_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building user profiles we can clearly analyze each user favourite ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ingredients = ut.get_top_favorite_ingredients(user_profiles, 813262) \n",
    "print(top_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also employ the same strategy to find out each community favourite ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_favorite_per_community = ut.get_top_favorite_ingredients_per_community(user_profiles, filtered_communities)\n",
    "for community, top_ingredients in top_favorite_per_community.items():\n",
    "    print(\"Top favorite ingredients for Community\", community, \":\")\n",
    "    print(top_ingredients)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data we can now find out the favourite recipes of each community and rank them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_recipes contains the recipes DataFrame and top_favorite_per_community is the dictionary containing top favorite ingredients for each community\n",
    "recommendations_per_community = ut.community_recipe_recommendations(df_recipes, top_favorite_per_community)\n",
    "for community, recommendations in recommendations_per_community.items():\n",
    "    print(\"Recommendations for Community\", community, \":\")\n",
    "    print(recommendations)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These favourite recipes can be recommended to the users of that own community, constituting a community-profile content-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_10, recall_at_10 = ut.evaluate_recommendations(recommendations_per_community, df_reviews, filtered_communities)\n",
    "print(\"Average Precision@10:\", precision_at_10)\n",
    "print(\"Average Recall@10:\", recall_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a term of comparison, we will also use a random recommender. This model will recommend random recipes to users who do not have enough reviews to make personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_reviews[['member_id', 'recipe_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_algo = NormalPredictor()\n",
    "rmse, mae, predictions, precision, recall = ut.evaluate_model(random_algo, trainset, testset)\n",
    "\n",
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage RMSE ->\\033[0m\", rmse)\n",
    "print(f\"\\033[1mAverage MAE ->\\033[0m\", mae)\n",
    "print(f\"\\033[1mAverage Precision@10 ->\\033[0m\", precision)\n",
    "print(f\"\\033[1mAverage Recall@10 ->\\033[0m\", recall)\n",
    "print()    \n",
    "\n",
    "whole_dataset_rmse[\"Random Recommender\"] = rmse\n",
    "whole_dataset_mae[\"Random Recommender\"] = mae\n",
    "whole_dataset_precision[\"Random Recommender\"] = precision\n",
    "whole_dataset_recall[\"Random Recommender\"] = recall\n",
    "models_predictions[\"Random Recommender\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run random for communities\n",
    "avg_rmse, avg_mae, avg_precision, avg_recall = ut.collaborative_filtering(df_reviews, filtered_communities, model_type='Random')\n",
    "communities_rmse[\"Random Recommender\"] = avg_rmse\n",
    "communities_mae[\"Random Recommender\"] = avg_mae\n",
    "communities_precision[\"Random Recommender\"] = avg_precision\n",
    "communities_recall[\"Random Recommender\"] = avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering (Applied @ whole data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubcf_algo = KNNBasic(sim_options={'user_based': True}, verbose=False)\n",
    "rmse, mae, pred_user, precision, recall = ut.evaluate_model(ubcf_algo, trainset, testset)\n",
    "\n",
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage RMSE ->\\033[0m\", avg_rmse)\n",
    "print(f\"\\033[1mAverage MAE ->\\033[0m\", avg_mae)\n",
    "print(f\"\\033[1mAverage Precision@10 ->\\033[0m\", avg_precision)\n",
    "print(f\"\\033[1mAverage Recall@10 ->\\033[0m\", avg_recall)\n",
    "print()    \n",
    "\n",
    "whole_dataset_rmse[\"User-Based CF\"] = rmse\n",
    "whole_dataset_mae[\"User-Based CF\"] = mae\n",
    "whole_dataset_precision[\"User-Based CF\"] = precision\n",
    "whole_dataset_recall[\"User-Based CF\"] = recall\n",
    "models_predictions[\"User-Based CF\"] = pred_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Item-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibcf_algo = KNNBasic(sim_options={'user_based': False}, verbose=False)\n",
    "rmse, mae, pred_item, precision, recall = ut.evaluate_model(ibcf_algo, trainset, testset)\n",
    "\n",
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage RMSE ->\\033[0m\", rmse)\n",
    "print(f\"\\033[1mAverage MAE ->\\033[0m\", mae)\n",
    "print(f\"\\033[1mAverage Precision@10 ->\\033[0m\", precision)\n",
    "print(f\"\\033[1mAverage Recall@10 ->\\033[0m\", recall)\n",
    "print()    \n",
    "\n",
    "whole_dataset_rmse[\"Item-Based CF\"] = rmse\n",
    "whole_dataset_mae[\"Item-Based CF\"] = mae\n",
    "whole_dataset_precision[\"Item-Based CF\"] = precision\n",
    "whole_dataset_recall[\"Item-Based CF\"] = recall\n",
    "models_predictions[\"Item-Based CF\"] = pred_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_algo = SVD(verbose = False)\n",
    "rmse, mae, pred_model, precision, recall = ut.evaluate_model(ibcf_algo, trainset, testset)\n",
    "\n",
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage RMSE ->\\033[0m\", rmse)\n",
    "print(f\"\\033[1mAverage MAE ->\\033[0m\", mae)\n",
    "print(f\"\\033[1mAverage Precision@10 ->\\033[0m\", precision)\n",
    "print(f\"\\033[1mAverage Recall@10 ->\\033[0m\", recall)\n",
    "print()  \n",
    "\n",
    "whole_dataset_rmse[\"Model-Based CF\"] = rmse\n",
    "whole_dataset_mae[\"Model-Based CF\"] = mae\n",
    "whole_dataset_precision[\"Model-Based CF\"] = precision\n",
    "whole_dataset_recall[\"Model-Based CF\"] = recall\n",
    "models_predictions[\"Model-Based CF\"] = pred_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Filtering (Applied @ whole data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision, avg_recall = ut.overall_content_based_filtering(df_reviews, df_similar_recipes, filtered_communities)\n",
    "\n",
    "print(f\"\\033[1m-----Overall Performance-----\\033[0m\")\n",
    "print(f\"\\033[1mAverage Precision ->\\033[0m\", avg_precision)\n",
    "print(f\"\\033[1mAverage Recall ->\\033[0m\", avg_recall)\n",
    "print()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE and MAE values for each model with communities and whole dataset\n",
    "# Models\n",
    "models = [\"Random Recommender\", \n",
    "          \"User-Based CF\", \n",
    "          \"Item-Based CF\",\n",
    "          \"Model-Based CF\", \n",
    "          \"Content-Based Filtering\"\n",
    "          ]\n",
    "indices = np.arange(len(models))\n",
    "\n",
    "# RMSE and MAE values for each model with communities and whole dataset\n",
    "communities_rmse_values = [communities_rmse[model] for model in models]\n",
    "communities_mae_values = [communities_mae[model] for model in models]\n",
    "communities_precision_values = [communities_precision[model] for model in models]\n",
    "communities_recall_values = [communities_recall[model] for model in models]\n",
    "whole_dataset_rmse_values = [whole_dataset_rmse[model] for model in models]\n",
    "whole_dataset_mae_values = [whole_dataset_mae[model] for model in models]\n",
    "whole_dataset_precision_values = [whole_dataset_precision[model] for model in models]\n",
    "whole_dataset_recall_values = [whole_dataset_recall[model] for model in models]\n",
    "\n",
    "\n",
    "# Create subplots for RMSE and MAE values\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(12, 25))\n",
    "bar_width = 0.4\n",
    "\n",
    "\n",
    "# Plot RMSE values\n",
    "for i, value in enumerate(communities_rmse_values):\n",
    "    axes[0].text(i - bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "    \n",
    "for i, value in enumerate(whole_dataset_rmse_values):\n",
    "    axes[0].text(i + bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "    \n",
    "axes[0].bar(indices - 0.2, communities_rmse_values, width=bar_width, color='lightsalmon', alpha=0.6, label='Communities RMSE')\n",
    "axes[0].bar(indices + 0.2, whole_dataset_rmse_values, width=bar_width, color='wheat', alpha=0.6, label='Whole Dataset RMSE')\n",
    "axes[0].set_xticks(indices)\n",
    "axes[0].set_xticklabels(models, rotation=30)\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE Values')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[0].set_ylim(0, max(communities_rmse_values + whole_dataset_rmse_values) + 0.3)\n",
    "\n",
    "# Plot MAE values\n",
    "for i, value in enumerate(communities_mae_values):\n",
    "    axes[1].text(i - bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "    \n",
    "for i, value in enumerate(whole_dataset_mae_values):\n",
    "    axes[1].text(i + bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "    \n",
    "axes[1].bar(indices - 0.2, communities_mae_values, width=bar_width, color='cornflowerblue', alpha=0.6, label='Communities MAE')\n",
    "axes[1].bar(indices + 0.2, whole_dataset_mae_values, width=bar_width, color='seagreen', alpha=0.6, label='Whole Dataset MAE')\n",
    "axes[1].set_xticks(indices)\n",
    "axes[1].set_xticklabels(models, rotation=30)\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE Values')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[1].set_ylim(0, max(communities_mae_values + whole_dataset_mae_values) + 0.3)\n",
    "\n",
    "# Plot Precision values\n",
    "for i, value in enumerate(communities_precision_values):\n",
    "    axes[2].text(i - bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "\n",
    "for i, value in enumerate(whole_dataset_precision_values):\n",
    "    axes[2].text(i + bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "\n",
    "axes[2].bar(indices - 0.2, communities_precision_values, width=bar_width, color='lightsalmon', alpha=0.6, label='Communities Precision')\n",
    "axes[2].bar(indices + 0.2, whole_dataset_precision_values, width=bar_width, color='wheat', alpha=0.6, label='Whole Dataset Precision')\n",
    "axes[2].set_xticks(indices)\n",
    "axes[2].set_xticklabels(models, rotation=30)\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Precision Values')\n",
    "axes[2].legend()\n",
    "\n",
    "axes[2].set_ylim(0, max(communities_precision_values + whole_dataset_precision_values) + 0.3)\n",
    "\n",
    "# Plot Recall values\n",
    "for i, value in enumerate(communities_recall_values):\n",
    "    axes[3].text(i - bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "    \n",
    "for i, value in enumerate(whole_dataset_recall_values):\n",
    "    axes[3].text(i + bar_width/2, value - 0.01, str(round(value, 3)), ha='center', va='top')\n",
    "    \n",
    "axes[3].bar(indices - 0.2, communities_recall_values, width=bar_width, color='cornflowerblue', alpha=0.6, label='Communities Recall')\n",
    "axes[3].bar(indices + 0.2, whole_dataset_recall_values, width=bar_width, color='seagreen', alpha=0.6, label='Whole Dataset Recall')\n",
    "axes[3].set_xticks(indices)\n",
    "axes[3].set_xticklabels(models, rotation=30)\n",
    "axes[3].set_ylabel('Recall')\n",
    "axes[3].set_title('Recall Values')\n",
    "axes[3].legend()\n",
    "\n",
    "axes[3].set_ylim(0, max(communities_recall_values + whole_dataset_recall_values) + 0.3)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "After analysing the results of the different models, we can conclude that the best model for our dataset is the collaborative filtering model, more specifically Model-Based. This model achieved the lowest RMSE and MAE values, indicating that it is the most accurate model for making recommendations to users based on the recipes they have reviewed.\n",
    "\n",
    "This was expected as our dataset is very sparse, and the memory-based models are not expected to perform well in this scenario. The content-based model also performed well, but it is limited by the features available in the dataset, and it may not be able to capture all the nuances of the users' tastes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
